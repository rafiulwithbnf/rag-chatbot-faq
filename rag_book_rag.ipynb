{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f7789-b761-461e-9330-018e1ff59948",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: D:\\1iNstallation-Files\\envs\\ragbook\\python.exe\n",
      "Requirement already satisfied: pip in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (25.2)\n",
      "Requirement already satisfied: sentence-transformers in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: faiss-cpu in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: pypdf in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: requests in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: numpy in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: tqdm in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (4.56.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (0.35.0)\n",
      "Requirement already satisfied: Pillow in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: colorama in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from ipywidgets) (9.5.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 8.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.3 MB/s  0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   ---------------------------------------- 3/3 [ipywidgets]\n",
      "\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook qtconsole run script server\n",
      "troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python executable:\", sys.executable)\n",
    "\n",
    "# Install packages into the active kernel environment\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install sentence-transformers faiss-cpu pypdf requests numpy tqdm\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab6e01a-dd40-4332-bbf9-92f824658fca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Q&A chunks: 45\n",
      "\n",
      "---\n",
      "ID: e6d891b8-d003-407e-9187-246f9e77aba7\n",
      "Num: 1. Pages: None - None\n",
      "Q: Are you guy’s insurance company or Broker?\n",
      "A: Not really, we are neither an insurance company nor a broker. Basically, we are an independent price searching company, search on your behalf with your consent to find you reasonable online prices. We do not work as a broker or agent for any specific company but a Non-Regulated Introducer for 'Kindertons Limited' (FRN 306969). and 'Worldin Holding Limited' who are authorised and regulated by Financial Conduct Authority (FCA). However, we are registered with the Information Commissioner's Office (ICO) and all your information are protected under the 'Data Protection Act & General Data Protection Regulation (GDPR 2018). We can assist the valuable customers with very helpful service as our multilingual dedicated friendly staffs are waiting to serve. Also, you are strongly advised to read our Terms and Conditions and Privacy Policy thoroughly before accepting our services.\n",
      "\n",
      "---\n",
      "ID: 330a25ed-4690-4166-8779-99c541c6fca8\n",
      "Num: 2. Pages: None - None\n",
      "Q: How do you get the prices so cheap?\n",
      "A: - Well’ it’s a really good question. Basically, there are no secrets or special tricks it’s just we spend our full time behind it. And as you might not have enough information of the market or time to spend all your day. For example, we search at least 5-7 comparison sites along with the online and offline markets even over the phones where you search maximum of 2/3 sites. And trust me that really take quite a lot of time & stress to find maximum possible quotations for our valuable customers throughout the industry.\n",
      "\n",
      "---\n",
      "ID: a2bc0879-ecd5-4511-ad66-31aebd2587df\n",
      "Num: 3. Pages: None - None\n",
      "Q: What is MOT (Never asked by customer but you must know)?\n",
      "A: * -MOT is a compulsory annual test taken by the ‘’Ministry of Transport’’ for safety and exhaust emissions of motor vehicles of more than a specified age (most vehicles over three years old). And no MOT test is required from the vehicle registration date to three years period.\n",
      "\n",
      "---\n",
      "ID: aaa62809-aef1-4de4-86c9-f06688a820a3\n",
      "Num: 4. Pages: None - None\n",
      "Q: What is MID (Never asked by customer but you must know)?\n",
      "A: * -MID means Motor insurance database which is a centralised database of motor insurance policy information of all insured UK vehicles. Please go on www.mid.com\n",
      "\n",
      "---\n",
      "ID: 77e744a5-ad86-4e82-ba0a-d022f36ccad2\n",
      "Num: 5. Pages: None - None\n",
      "Q: What is Write off or Written Off (Never asked by customer but you must know)?\n",
      "A: * - A written off car means a car that has suffered damage as a result of a crash and the insurer considers the cost of repairs to be uneconomical- usually around 50% of the vehicle’s value.\n",
      "\n",
      "---\n",
      "ID: 2b251f04-3ca6-4145-afbb-b62345cc8841\n",
      "Num: 6. Pages: 2 - 2\n",
      "Q: What are the common motoring convictions in UK (Never asked by customer but you must know - I repeat, you must know)?\n",
      "A: * IN10 for driving without insurance (expected points 6 and fines £250+), SP30 for speeding on local road (expected points 3 and fines £60+), SP50 for speeding on Motorway (expected points 3 and fines £100+), CU80 for using mobile phones while driving (expected points 6 and fines £100+), TS10 for red signals violation (expected points 3 and fines £60+), ETC.\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "pdf_path = r\"D:\\BnF Tech\\RAG\\FAQ Updated.pdf\"  # full path to the PDF file\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "pages = []\n",
    "for i, page in enumerate(reader.pages, start=1):\n",
    "    txt = page.extract_text()\n",
    "    pages.append({\n",
    "        \"page\": i,\n",
    "        \"text\": txt if txt else \"\"\n",
    "    })\n",
    "\n",
    "#print(f\"Total pages read: {len(pages)}\")\n",
    "#for page in pages:\n",
    "    #print(f\"\\n--- Page {page['page']} ---\\n\")\n",
    "    #print(page[\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Robust Q&A chunker for FAQ PDF where answers may span pages.\n",
    "import re, uuid\n",
    "from pprint import pprint\n",
    "\n",
    "def build_full_text_with_page_markers(pages):\n",
    "    # pages is list of {\"page\": i, \"text\": ...}\n",
    "    full = []\n",
    "    for p in pages:\n",
    "        marker = f\"\\n<<PAGE_{p['page']}>>\\n\"\n",
    "        full.append(marker + (p['text'] or \"\"))\n",
    "    return \"\".join(full)\n",
    "\n",
    "def qa_chunks_from_pages(pages):\n",
    "    full = build_full_text_with_page_markers(pages)\n",
    "\n",
    "    # Pattern: find blocks that start with a number and dot (e.g., \"1.\", \"23.\")\n",
    "    # DOTALL so '.' matches newlines. We keep the block from one number to the next number or EOF.\n",
    "    pattern = re.compile(r'(?s)(?:^|\\n)\\s*(\\d{1,3}\\.)\\s*(.*?)(?=(?:\\n\\s*\\d{1,3}\\.)|\\Z)')\n",
    "    chunks = []\n",
    "\n",
    "    for m in pattern.finditer(full):\n",
    "        num = m.group(1).strip()          # e.g., \"1.\"\n",
    "        block = m.group(2).strip() or \"\"  # the text after the number until next number\n",
    "\n",
    "        # Find which pages this block spans by searching for page markers inside the match span\n",
    "        block_span = full[m.start():m.end()]\n",
    "        pages_found = re.findall(r'<<PAGE_(\\d+)>>', block_span)\n",
    "        if pages_found:\n",
    "            page_start = int(pages_found[0])\n",
    "            page_end = int(pages_found[-1])\n",
    "        else:\n",
    "            page_start = page_end = None\n",
    "\n",
    "        # Clean block: remove inline page markers to simplify splitting\n",
    "        block_clean = re.sub(r'<<PAGE_\\d+>>', ' ', block)\n",
    "\n",
    "        # Heuristic to split question vs answer:\n",
    "        # 1) If '**' appears (your FAQ uses **), split at first '**' occurrence\n",
    "        # 2) Else, if there's a '?' within the first 200 chars, split at first '?'\n",
    "        # 3) Else split at first newline (reasonable fallback)\n",
    "        q_text = \"\"\n",
    "        a_text = \"\"\n",
    "        if '**' in block_clean:\n",
    "            # split on the first occurrence of '**' (or multiple)\n",
    "            parts = block_clean.split('**', 1)\n",
    "            # If the pattern is \"Q ... **  - Answer...\" often the question is before **\n",
    "            # But sometimes ** marks the question or the \"answer start\", so we'll test lengths\n",
    "            if len(parts[0].strip()) < 400:  # treat left part as question if not huge\n",
    "                q_text = parts[0].strip()\n",
    "                a_text = parts[1].strip()\n",
    "            else:\n",
    "                # fallback: take first sentence as question\n",
    "                first_q = re.split(r'(?<=[\\?\\.\\!])\\s', block_clean, maxsplit=1)\n",
    "                q_text = first_q[0].strip()\n",
    "                a_text = first_q[1].strip() if len(first_q) > 1 else \"\"\n",
    "        else:\n",
    "            # find first '?' in the first N chars\n",
    "            head = block_clean[:600]  # look in first 600 chars for a question mark\n",
    "            qm = head.find('?')\n",
    "            if qm != -1:\n",
    "                # split at the first question mark (include it)\n",
    "                q_text = block_clean[:qm+1].strip()\n",
    "                a_text = block_clean[qm+1:].strip()\n",
    "            else:\n",
    "                # fallback: split at the first newline that seems to separate the answer\n",
    "                parts = re.split(r'\\n\\s*\\n', block_clean, maxsplit=1)  # blank-line separator\n",
    "                if len(parts) > 1:\n",
    "                    q_text = parts[0].strip()\n",
    "                    a_text = parts[1].strip()\n",
    "                else:\n",
    "                    # very last fallback: try first sentence\n",
    "                    first_sentence = re.split(r'(?<=[\\.\\!\\?])\\s', block_clean, maxsplit=1)\n",
    "                    q_text = first_sentence[0].strip()\n",
    "                    a_text = first_sentence[1].strip() if len(first_sentence) > 1 else \"\"\n",
    "\n",
    "        # Final cleanup: remove excessive whitespace\n",
    "        q_text = re.sub(r'\\s+', ' ', (q_text or \"\")).strip()\n",
    "        a_text = re.sub(r'\\s+', ' ', (a_text or \"\")).strip()\n",
    "\n",
    "        # If the \"question\" we detected is empty, try to reconstruct as \"Question <num>\"\n",
    "        if not q_text:\n",
    "            q_text = f\"{num} (Question text not detected)\"\n",
    "        # If answer empty, set explicit note (so retrieval can show it's incomplete)\n",
    "        if not a_text:\n",
    "            a_text = \"(answer not found in block - may be on adjacent pages)\"\n",
    "\n",
    "        chunk = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"number\": num,\n",
    "            \"page_start\": page_start,\n",
    "            \"page_end\": page_end,\n",
    "            \"question\": q_text,\n",
    "            \"answer\": a_text,\n",
    "            \"text\": f\"Question: {q_text}\\nAnswer: {a_text}\"\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Run the chunker on your `pages` (you already have pages from PdfReader)\n",
    "qa_chunks = qa_chunks_from_pages(pages)\n",
    "print(\"Detected Q&A chunks:\", len(qa_chunks))\n",
    "# show first few nicely\n",
    "for c in qa_chunks[:6]:\n",
    "    print(\"\\n---\")\n",
    "    print(\"ID:\", c[\"id\"])\n",
    "    print(\"Num:\", c[\"number\"], \"Pages:\", c[\"page_start\"], \"-\", c[\"page_end\"])\n",
    "    print(\"Q:\", c[\"question\"])\n",
    "    print(\"A:\", c[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0afa9f-916a-4dec-956b-d89f00b1bc38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (4.56.1)\n",
      "Requirement already satisfied: tqdm in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (0.35.0)\n",
      "Requirement already satisfied: Pillow in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\1installation-files\\envs\\ragbook\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd8f3dc9-42be-4b23-b1f0-d8221e42f641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built with 45 vectors and saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell — build FAISS index from qa_chunks\n",
    "import faiss, pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Make sure qa_chunks exists from your previous code\n",
    "assert \"qa_chunks\" in globals(), \"qa_chunks not found — run your chunking code first.\"\n",
    "\n",
    "EMB_MODEL = \"all-MiniLM-L6-v2\"  # lightweight & fast model\n",
    "embedder = SentenceTransformer(EMB_MODEL)\n",
    "\n",
    "# Extract texts (Q + A together) for embeddings\n",
    "texts = [c[\"text\"] for c in qa_chunks]\n",
    "embeddings = embedder.encode(texts, convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "# Build FAISS index\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save index\n",
    "faiss.write_index(index, \"qa_book_faiss.index\")\n",
    "\n",
    "# Save meta (ids, page ranges, Q/A)\n",
    "metadatas = [{\"id\": c[\"id\"], \"question\": c[\"question\"], \"answer\": c[\"answer\"],\n",
    "              \"page_start\": c[\"page_start\"], \"page_end\": c[\"page_end\"]} for c in qa_chunks]\n",
    "with open(\"qa_book_meta.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"texts\": texts, \"metadatas\": metadatas}, f)\n",
    "\n",
    "print(f\"FAISS index built with {index.ntotal} vectors and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0082a763-b21b-4ca7-b412-676e8c6bf6af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS index with vectors: 45\n",
      "Loaded meta entries: 45\n",
      "\n",
      "Example metadata[0]:\n",
      "{'id': 'e6d891b8-d003-407e-9187-246f9e77aba7', 'question': 'Are you guy’s insurance company or Broker?', 'answer': \"Not really, we are neither an insurance company nor a broker. Basically, we are an independent price searching company, search on your behalf with your consent to find you reasonable online prices. We do not work as a broker or agent for any specific company but a Non-Regulated Introducer for 'Kindertons Limited' (FRN 306969). and 'Worldin Holding Limited' who are authorised and regulated by Financial Conduct Authority (FCA). However, we are registered with the Information Commissioner's Office (ICO) and all your information are protected under the 'Data Protection Act & General Data Protection Regulation (GDPR 2018). We can assist the valuable customers with very helpful service as our multilingual dedicated friendly staffs are waiting to serve. Also, you are strongly advised to read our Terms and Conditions and Privacy Policy thoroughly before accepting our services.\", 'page_start': None, 'page_end': None}\n",
      "\n",
      "Example text excerpt:\n",
      "Question: Are you guy’s insurance company or Broker? Answer: Not really, we are neither an insurance company nor a broker. Basically, we are an independent price searching company, search on your behalf with your consent to find you reasonable online prices. We do not work as a broker or agent for any specific company but a Non-Regulated Introducer for 'Kindertons Limited' (FRN 306969). and 'Worldin Holding Limited' who are authorised and regulated by Financial Conduct Authority (FCA). However, we are registered with the Information Commissioner's Office (ICO) and all your information are protected under the 'Data Protection Act & General Data Protection Regulation (GDPR 2018). We can assist the valuable customers with very helpful service as our multilingual dedicated friendly staffs are waiting to serve. Also, you are strongly advised to read our Terms and Conditions and Privacy Policy thoroughly before accepting our services.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — load index & metadata\n",
    "import faiss, pickle, os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "FAISS_INDEX_FILE = \"qa_book_faiss.index\"\n",
    "META_FILE = \"qa_book_meta.pkl\"\n",
    "EMB_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "assert os.path.exists(FAISS_INDEX_FILE), \"FAISS index not found. Run the build cell first.\"\n",
    "assert os.path.exists(META_FILE), \"Meta file not found. Run the build cell first.\"\n",
    "\n",
    "index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "with open(META_FILE, \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "texts = meta[\"texts\"]\n",
    "metadatas = meta[\"metadatas\"]\n",
    "\n",
    "embedder = SentenceTransformer(EMB_MODEL)\n",
    "\n",
    "print(\"Loaded FAISS index with vectors:\", index.ntotal)\n",
    "print(\"Loaded meta entries:\", len(metadatas))\n",
    "# show top 1 example\n",
    "print(\"\\nExample metadata[0]:\")\n",
    "print(metadatas[0])\n",
    "print(\"\\nExample text excerpt:\")\n",
    "print(texts[0].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1233fa32-2aa1-4e6b-8080-4794120f7adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 (score=0.8083) pages None-None ---\n",
      "Q: Will a provisional driving licence holder be able to buy insurance?\n",
      "A excerpt: * - Yes sure they can but the UK driving law requires them to drive under supervision. A U k manual driver must be with him who is aged over 21 and license is 3 years old also an ‘L Plate/sign’ to be displayed at the front and back of the car.\n",
      "\n",
      "--- Result 2 (score=1.0004) pages None-None ---\n",
      "Q: What is a provisional Licence UK?\n",
      "A excerpt: ** You must have a provisional driving licence for Great Britain or Northern Ireland when you're learning to drive or ride. You must be supervised when you're learning to drive a car. This can be by a driving instructor or someone else who meets the rules, for example family or friends.\n",
      "\n",
      "--- Result 3 (score=1.0708) pages None-None ---\n",
      "Q: Will I be able to drive another car?\n",
      "A excerpt: * - Well’ it depends on the insurance companies mainly, if the driver meets certain rules which is --- - The driver Has to be a Policy Holder not the 2nd driver - Has to be aged over 25 - Have to have a Full UK/manual driving Licence for at least 1 year. - Job is not related to motor industry or taxi Driving. - However, you will find it on the certificate soon after you are covered by certain insu\n",
      "\n",
      "--- Result 4 (score=1.1640) pages 2-2 ---\n",
      "Q: What are the common motoring convictions in UK (Never asked by customer but you must know - I repeat, you must know)?\n",
      "A excerpt: * IN10 for driving without insurance (expected points 6 and fines £250+), SP30 for speeding on local road (expected points 3 and fines £60+), SP50 for speeding on Motorway (expected points 3 and fines £100+), CU80 for using mobile phones while driving (expected points 6 and fines £100+), TS10 for red signals violation (expected points 3 and fines £60+), ETC.\n",
      "\n",
      "--- Result 5 (score=1.1963) pages None-None ---\n",
      "Q: What type of cover is included in comprehensive insurance?\n",
      "A excerpt: ** Generally speaking you can expect most comprehensive car insurance policies to include  Accidental damage to your car or another's car  Personal injury to you or injury to other people involved in an accident  Cover for personal belongings that are damaged or stolen  Repairs to your windscreen\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — retrieval utility\n",
    "import numpy as np\n",
    "\n",
    "def retrieve(question, k=4):\n",
    "    # create embedding (float32 for faiss)\n",
    "    q_emb = embedder.encode([question], convert_to_numpy=True).astype(\"float32\")\n",
    "    D, I = index.search(q_emb, k)\n",
    "    results = []\n",
    "    for dist, idx in zip(D[0], I[0]):\n",
    "        results.append({\n",
    "            \"score\": float(dist),\n",
    "            \"text\": texts[idx],\n",
    "            \"meta\": metadatas[idx]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Quick interactive test (edit question as needed)\n",
    "q = \"what are the conditions for a provisional drivers insurance??\"\n",
    "res = retrieve(q, k=5)\n",
    "for i, r in enumerate(res, 1):\n",
    "    m = r[\"meta\"]\n",
    "    print(f\"\\n--- Result {i} (score={r['score']:.4f}) pages {m['page_start']}-{m['page_end']} ---\")\n",
    "    print(\"Q:\", m[\"question\"])\n",
    "    print(\"A excerpt:\", m[\"answer\"][:400].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3059c7a5-dbf7-445a-a988-69bac88bf017",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m     38\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mAre you an insurance company or broker?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m retrieved = \u001b[43mretrieve\u001b[49m(question, k=\u001b[32m4\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRetrieved passages titles:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m retrieved:\n",
      "\u001b[31mNameError\u001b[39m: name 'retrieve' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 3 — Ollama synthesis (optional)\n",
    "import requests, textwrap, json\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"llama3.2:3b\"   # change if you use a different small model\n",
    "TIMEOUT = 60\n",
    "\n",
    "def synthesize_with_ollama(question, retrieved, max_tokens=256):\n",
    "    # build context from retrieved meta (question + answer)\n",
    "    context = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"(pages {r['meta']['page_start']}-{r['meta']['page_end']}) Q: {r['meta']['question']}\\nA: {textwrap.shorten(r['meta']['answer'], 900, placeholder=' ...')}\"\n",
    "        for r in retrieved\n",
    "    ])\n",
    "    prompt = f\"\"\"You are an assistant that MUST answer using ONLY the provided passages. If the answer is not in the passages, reply: \"I don't know from the provided text.\"\n",
    "\n",
    "Passages:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer concisely and cite which page range(s) you used.\n",
    "\"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    "    resp = requests.post(OLLAMA_URL, json=payload, timeout=TIMEOUT)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    # Try common response keys (adjust if your Ollama returns different)\n",
    "    if isinstance(data, dict):\n",
    "        return data.get(\"response\") or data.get(\"output\") or json.dumps(data, indent=2)\n",
    "    return str(data)\n",
    "\n",
    "# Example usage:\n",
    "question = \"Are you an insurance company or broker?\"\n",
    "retrieved = retrieve(question, k=4)\n",
    "print(\"\\nRetrieved passages titles:\")\n",
    "for r in retrieved:\n",
    "    print(\"-\", r[\"meta\"][\"question\"], f\"(pages {r['meta']['page_start']}-{r['meta']['page_end']})\")\n",
    "print(\"\\nRequesting Ollama to synthesize...\\n\")\n",
    "try:\n",
    "    print(synthesize_with_ollama(question, retrieved))\n",
    "except Exception as e:\n",
    "    print(\"Ollama request failed:\", e)\n",
    "    # optionally print raw response if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad4a3f95-0758-4109-857f-6628c3d3febc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP 200\n",
      "----- resp.text preview -----\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:24.0278575Z\",\"response\":\"Hello\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:24.1931391Z\",\"response\":\"!\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:24.3504356Z\",\"response\":\" It\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:24.5105306Z\",\"response\":\"'s\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:24.6691414Z\",\"response\":\" nice\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:24.8287099Z\",\"response\":\" to\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:24.9970772Z\",\"response\":\" meet\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:25.1580679Z\",\"response\":\" you\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:25.3202097Z\",\"response\":\".\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:25.4896304Z\",\"response\":\" Is\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:25.6638606Z\",\"response\":\" there\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:25.8246056Z\",\"response\":\" something\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:25.9870703Z\",\"response\":\" I\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:26.1625577Z\",\"response\":\" can\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:26.3254362Z\",\"response\":\" help\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:26.4868909Z\",\"response\":\" you\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:26.6514661Z\",\"response\":\" with\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:26.8095801Z\",\"response\":\",\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:26.967795Z\",\"response\":\" or\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:27.1313586Z\",\"response\":\" would\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:27.2935127Z\",\"response\":\" you\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:27.5019685Z\",\"response\":\" like\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:27.675541Z\",\"response\":\" to\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:27.8384404Z\",\"response\":\" chat\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:28.0029573Z\",\"response\":\"?\",\"done\":false}\n",
      "{\"model\":\"llama3.2:3b\",\"created_at\":\"2025-09-22T09:37:28.169161Z\",\"response\":\"\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,9125,128007,271,38766,1303,33025,2696,25,6790,220,2366,18,271,128009,128006,882,128007,271,15339,128009,128006,78191,128007,271,9906,0,1102,596,6555,311,3449,499,13,2209,1070,2555,358,649,1520,499,449,11,477,1053,499,1093,311,6369,30],\"total_duration\":7295632600,\"load_duration\":301876900,\"prompt_eval_count\":26,\"prompt_eval_duration\":2845866900,\"eval_count\":26,\"eval_duration\":4146500600}\n",
      "\n",
      "----- end preview -----\n"
     ]
    }
   ],
   "source": [
    "import requests, json, textwrap\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "payload = {\"model\": \"llama3.2:3b\", \"prompt\": \"hello\", \"max_tokens\": 16}\n",
    "\n",
    "try:\n",
    "    r = requests.post(OLLAMA_URL, json=payload, timeout=30)\n",
    "    print(\"HTTP\", r.status_code)\n",
    "    print(\"----- resp.text preview -----\")\n",
    "    print(r.text[:4000])   # print first 4000 chars\n",
    "    print(\"----- end preview -----\")\n",
    "except Exception as e:\n",
    "    print(\"Request failed:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e433615-3742-4a12-b3bc-fd70efd58535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test retrieval results:\n",
      "- Are you guy’s insurance company or Broker? (pages None - None )\n",
      "- Do you do taxi Insurance? (pages None - None )\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss, pickle\n",
    "\n",
    "# Load FAISS + meta if not already loaded\n",
    "FAISS_INDEX_FILE = \"qa_book_faiss.index\"\n",
    "META_FILE = \"qa_book_meta.pkl\"\n",
    "\n",
    "if \"index\" not in globals():\n",
    "    index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "if \"meta\" not in globals():\n",
    "    with open(META_FILE, \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "\n",
    "texts = meta[\"texts\"]\n",
    "metadatas = meta[\"metadatas\"]\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def retrieve(query, k=4):\n",
    "    \"\"\"Search FAISS index and return top-k matches with their metadata.\"\"\"\n",
    "    query_emb = embedder.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    D, I = index.search(query_emb, k)\n",
    "    results = []\n",
    "    for i, idx in enumerate(I[0]):\n",
    "        if idx < 0 or idx >= len(texts):\n",
    "            continue\n",
    "        results.append({\n",
    "            \"score\": float(D[0][i]),\n",
    "            \"text\": texts[idx],\n",
    "            \"meta\": metadatas[idx]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Quick test\n",
    "print(\"Test retrieval results:\")\n",
    "for r in retrieve(\"insurance company or broker?\", k=2):\n",
    "    print(\"-\", r[\"meta\"][\"question\"], \"(pages\", r[\"meta\"][\"page_start\"], \"-\", r[\"meta\"][\"page_end\"], \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db9cdf5-c3e6-4e99-b6d6-9834a60c9a66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person involved in the accident, along with their personal belongings' value, would be responsible for paying. However, \"if fault accident- please give him his insurer phone number\" suggests that if it's a fault accident, the insurance company will likely cover the costs. \n",
      "\n",
      "Since this question is answered on pages 7-7 (comprehensive cover), and also pages None-None (third party, fire and theft cover) mentions damage to other vehicles or people but not payment for damages, I don't know from the provided text.\n",
      "\n",
      "However, in page 3-3, when it's a non-fault accident, \"take his involved car registration number, involved driver name of his car at the time of accident, the place & time of accident, his right contact number then simply pass it over to the Kindertons for the rest and advise the client not to contact insurer\" implies that someone else will pay in case of non-fault.\n",
      "\n",
      "To answer more concisely: If non-fault, \"Kindertons\" would likely cover costs.\n"
     ]
    }
   ],
   "source": [
    "# Ollama NDJSON-safe synth (paste into a Jupyter cell)\n",
    "import requests, json, textwrap\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"llama3.2:3b\"\n",
    "TIMEOUT = 180  # allow cold-starts\n",
    "\n",
    "def ollama_generate_concat(prompt, model=MODEL_NAME, timeout=TIMEOUT, max_tokens=256):\n",
    "    payload = {\"model\": model, \"prompt\": prompt, \"max_tokens\": max_tokens, \"temperature\": 0.0}\n",
    "    try:\n",
    "        resp = requests.post(OLLAMA_URL, json=payload, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        text = resp.text\n",
    "        # Split by newlines and parse JSON per-line (NDJSON)\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "        pieces = []\n",
    "        last_obj = None\n",
    "        for ln in lines:\n",
    "            try:\n",
    "                obj = json.loads(ln)\n",
    "            except Exception:\n",
    "                # try to strip \"data:\" prefix if present\n",
    "                if ln.startswith(\"data:\"):\n",
    "                    try:\n",
    "                        obj = json.loads(ln[len(\"data:\"):].strip())\n",
    "                    except Exception:\n",
    "                        obj = None\n",
    "                else:\n",
    "                    obj = None\n",
    "            if obj is None:\n",
    "                continue\n",
    "            # some responses use \"response\" field per chunk\n",
    "            part = obj.get(\"response\") or obj.get(\"text\") or obj.get(\"output\")\n",
    "            if isinstance(part, dict):\n",
    "                # if nested, stringify it\n",
    "                part = json.dumps(part)\n",
    "            if part:\n",
    "                pieces.append(str(part))\n",
    "            last_obj = obj\n",
    "        # join pieces with no extra spaces (they are already token chunks)\n",
    "        joined = \"\".join(pieces).strip()\n",
    "        # if joined empty, attempt other fallbacks\n",
    "        if not joined and last_obj:\n",
    "            # try common keys in the last obj\n",
    "            for k in (\"response\", \"output\", \"result\", \"text\"):\n",
    "                if k in last_obj:\n",
    "                    joined = str(last_obj[k])\n",
    "                    break\n",
    "        return joined if joined else None\n",
    "    except Exception as e:\n",
    "        print(\"Ollama request failed:\", repr(e))\n",
    "        return None\n",
    "\n",
    "def synthesize_with_ollama_streaming_or_fallback(question, retrieved, max_tokens=256):\n",
    "    context = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"(pages {r['meta']['page_start']}-{r['meta']['page_end']}) Q: {r['meta']['question']}\\nA: {textwrap.shorten(r['meta']['answer'], 900, placeholder=' ...')}\"\n",
    "        for r in retrieved\n",
    "    ])\n",
    "    prompt = f\"\"\"You are an assistant that MUST answer using ONLY the provided passages. If the answer is not in the passages, reply: \"I don't know from the provided text.\"\n",
    "\n",
    "Passages:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer concisely and cite which page range(s) you used.\n",
    "\"\"\"\n",
    "    out = ollama_generate_concat(prompt, max_tokens=max_tokens)\n",
    "    if out:\n",
    "        return out\n",
    "    # fallback: return top retrieved answers\n",
    "    best = \"\\n\\n\".join([f\"Page {r['meta']['page_start']}-{r['meta']['page_end']}: {r['meta']['answer']}\" for r in retrieved[:3]])\n",
    "    return f\"(Fallback — top retrieved answers)\\n\\n{best}\"\n",
    "\n",
    "# Example prompt:\n",
    "q = \"?\"\n",
    "retrieved = retrieve(q, k=4)   \n",
    "print(synthesize_with_ollama_streaming_or_fallback(q, retrieved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49faa7b-b7bb-4908-85b3-217ef812c399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ragbook]",
   "language": "python",
   "name": "conda-env-ragbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
